{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5.1\n",
    "For the equation\n",
    "$f (x) = x^2 − x − 2 = 0$,\n",
    "each of the following functions yields an equivalent fixed-point problem:\n",
    "$$g_1 (x) =x^2 − 2,$$\n",
    "$$g_2 (x) = \\sqrt{x+2},$$\n",
    "$$g_3 (x) = 1 + \\frac{2}{x},$$\n",
    "$$g_4 (x) = \\frac{x^2 + 2}{2x - 1}.$$\n",
    "\n",
    "(a) Analyze the convergence properties of\n",
    "each of the corresponding fixed-point iteration\n",
    "schemes for the root x = 2 by considering\n",
    "$|g_{i}^{'}(2)|$.\n",
    "\n",
    "$$| g_{1}^{'}(2) | = 2x \\biggr|^{x=2} = 4$$\n",
    "We expect the scheme for $g_1(x)$ to diverge since the absolute value of the derivative is\n",
    "greater than 1.\n",
    "\n",
    "$$| g_{2}^{'}(2) | = \\frac{1}{2\\sqrt{x+2}}\\biggr|^{x=2} = \\frac{1}{4}$$\n",
    "We expect the scheme for $g_2(x)$ to be locally convergent since the absolute value of the derivative is\n",
    "less than 1. This means there exists an interval around x where the iterative scheme will converge\n",
    "if started in this interval.\n",
    "\n",
    "$$| g_{3}^{'}(2) | = \\frac{-2}{x^{2}}\\biggr|^{x=2} = 2$$\n",
    "We expect the scheme for $g_3(x)$ to diverge since the absolute value of the derivative is\n",
    "greater than 1.\n",
    "$$| g_{4}^{'}(2) | = \\frac{2x -2}{(2x - 1)^2}\\biggr|^{x=2} = \\frac{2}{9}$$\n",
    "We expect the scheme for $g_4(x)$ to be locally convergent since the absolute value of the derivative is\n",
    "less than 1. This means there exists an interval around x where the iterative scheme will converge\n",
    "if started in this interval.\n",
    "\n",
    "\n",
    "(b) Confirm your analysis by implementing\n",
    "each of the schemes and verifying its convergence (or lack thereof) and approximate convergence rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1, it diverges so it doesn't make sense to talk about convergence rates.\n",
      "\n",
      "CASE 2, it converges to 2.0 in 37 iterations. Since there are 15 decimals of precision the approximate rate of convergence is 0.40540540540540543\n",
      "\n",
      "CASE 3, it converges to 2.000001144409616 in 20 iterations. Since there are 6 decimals of precision the approximate rate of convergence is 0.3\n",
      "\n",
      "CASE 4, it diverges so it doesn't make sense to talk about convergence rates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Fixed point iteration convergence.\n",
    "\n",
    "The purposes is to evaluate the convergence / divergence of different\n",
    "solution functions.\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from math import sqrt, log2\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2 - x - 2\n",
    "\n",
    "\n",
    "def g1(x):\n",
    "    return x ** 2 - 2\n",
    "\n",
    "\n",
    "def g2(x):\n",
    "    return sqrt(x + 2)\n",
    "\n",
    "\n",
    "def g3(x):\n",
    "    return 1.0 + 2.0 / x\n",
    "\n",
    "\n",
    "def g4(x):\n",
    "    return (x ** 2 + 1) / (2 * x - 1)\n",
    "\n",
    "\n",
    "def evaluate(g, x):\n",
    "    current_x = x\n",
    "    previous_error = float(\"inf\")\n",
    "    current_error = f(x) ** 2\n",
    "    counter = 0\n",
    "    it = 0\n",
    "    while counter < 10:\n",
    "        current_x = g(current_x)\n",
    "        previous_error = current_error\n",
    "        current_error = f(current_x)\n",
    "        if previous_error <= current_error:\n",
    "            counter += 1\n",
    "        it += 1\n",
    "    return current_x, current_error, it\n",
    "\n",
    "\n",
    "x = 4\n",
    "\n",
    "print(\"CASE 1, it diverges so it doesn't make sense to talk about convergence rates.\\n\")\n",
    "#print(evaluate(g1, x))\n",
    "\n",
    "val_x, err, it = evaluate(g2, x)\n",
    "print(\"CASE 2, it converges to {0} in {1} iterations. Since there are\"\n",
    "      \" 15 decimals of precision the approximate rate of convergence is {2}\\n\".format(val_x, it, 15 / it))\n",
    "\n",
    "val_x, err, it = evaluate(g3, x)\n",
    "print(\"CASE 3, it converges to {0} in {1} iterations. Since there are\"\n",
    "      \" 6 decimals of precision the approximate rate of convergence is {2}\\n\".format(val_x, it, 6 / it))\n",
    "\n",
    "print(\"CASE 4, it diverges so it doesn't make sense to talk about convergence rates.\\n\")\n",
    "#print(evaluate(g4, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1 - bisection: The result is 2.0945520401000977 achieved in 22 iterations.\n",
      "CASE 1 - newton: The result is 2.094551342916024 achieved in 29 iterations.\n",
      "CASE 1 - secant: The result is 0.0 achieved in 1 iterations.\n",
      "\n",
      "CASE 2 - bisection: The result is 0.567143440246582 achieved in 22 iterations.\n",
      "CASE 2 - newton: The result is 0.567143175042986 achieved in 6 iterations.\n",
      "CASE 2 - secant: The result is 0.0 achieved in 1 iterations.\n",
      "\n",
      "CASE 3 - bisection: The result is 1.1141576766967773 achieved in 22 iterations.\n",
      "CASE 3 - newton: The result is 9.31724294141481 achieved in 1000 iterations.\n",
      "CASE 3 - secant: The result is 0.0 achieved in 1 iterations.\n",
      "\n",
      "CASE 4 - bisection: The result is 0.999995231628418 achieved in 22 iterations.\n",
      "CASE 4 - newton: The result is 0.9775747603079661 achieved in 1000 iterations.\n",
      "CASE 4 - secant: The result is 0.0 achieved in 1 iterations.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from math import copysign, exp, sin\n",
    "from scipy.misc import derivative\n",
    "\n",
    "def sign(x):\n",
    "    return copysign(1, x)\n",
    "\n",
    "\n",
    "def bisection(f, a, b, tolerance):\n",
    "    count = 0\n",
    "    while b - a > tolerance:\n",
    "        m = (b + a) / 2\n",
    "        if sign(f(a)) == sign(f(m)):\n",
    "            a = m\n",
    "        else:\n",
    "            b = m\n",
    "        count += 1\n",
    "    return m, count\n",
    "\n",
    "\n",
    "def newton(f, x, expected, tolerance):\n",
    "    count = 0\n",
    "    while abs(x - expected) > tolerance and count < 1000:\n",
    "        deriv = derivative(f, x)\n",
    "        x -= f(x) / deriv\n",
    "        count += 1\n",
    "    \n",
    "    return x, count\n",
    "\n",
    "\n",
    "def secant_method(f, x, expected, tolerance):\n",
    "    \"\"\"Doesn't work :(\"\"\"\n",
    "    count = 0\n",
    "    prev_x = 0\n",
    "    prev_f_x = 0\n",
    "    while abs(x - expected) > tolerance and count < 1000:\n",
    "        try:\n",
    "            x = x - f(x) * (x - prev_x) / (f(x) - prev_f_x)\n",
    "        except ZeroDivisionError:\n",
    "            return x, count\n",
    "        prev_x = x\n",
    "        prev_f_x = f(x)\n",
    "        count += 1\n",
    "    \n",
    "    return x, count\n",
    "\n",
    "\n",
    "# Functions for the various parts of the question.\n",
    "def fa(x):\n",
    "    return x ** 3 - 2 * x - 5\n",
    "\n",
    "def fb(x):\n",
    "    return exp(-x) - x\n",
    "\n",
    "def fc(x):\n",
    "    return x * sin(x) - 1\n",
    "\n",
    "def fd(x):\n",
    "    return x ** 3 - 3 * x ** 2 + 3 * x - 1\n",
    "\n",
    "def ez_print(case, typ, res):\n",
    "    print(\"CASE {} - {}: The result is {} achieved in {} iterations.\".format(case, typ, *res))\n",
    "\n",
    "# CASE (a)\n",
    "ez_print(1, \"bisection\", bisection(fa, 0, 4, 0.000001))\n",
    "ez_print(1, \"newton\", newton(fa, 0, 2.09455204, 0.000001))\n",
    "ez_print(1, \"secant\", secant_method(fa, 6, 2.09455204, 0.000001))\n",
    "print()\n",
    "# Case (b)\n",
    "ez_print(2, \"bisection\", bisection(fb, 0, 4, 0.000001))\n",
    "ez_print(2, \"newton\", newton(fb, 0, 0.567143440246582, 0.000001))\n",
    "ez_print(2, \"secant\", secant_method(fb, 6, 0.567143440246582, 0.000001))\n",
    "print()\n",
    "# Case (c)\n",
    "ez_print(3, \"bisection\", bisection(fc, 0, 4, 0.000001))\n",
    "ez_print(3, \"newton\", newton(fc, 10, 1.1141576766967773, 0.000001))\n",
    "ez_print(3, \"secant\", secant_method(fc, 6, 1.1141576766967773, 0.000001))\n",
    "print()\n",
    "\n",
    "# Case (d)\n",
    "ez_print(4, \"bisection\", bisection(fd, 0, 4, 0.000001))\n",
    "ez_print(4, \"newton\", newton(fd, 0, 0.999995231628418, 0.000001))\n",
    "ez_print(4, \"secant\", secant_method(fd, 6, 0.999995231628418, 0.000001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6.9\n",
    "\n",
    "What I've noticed so far, which makes sense considering the readings, is that steepest descent is very slow compared to the newton method of solving this problem.\n",
    "\n",
    "The damped newton method doesn't work very well, I think maybe I did code it correctly?\n",
    "\n",
    "## Not complete\n",
    "Plot the path taken in the plane\n",
    "by the approximate solutions for each method\n",
    "from each starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steepest Descent\n",
      "----------------\n",
      "For the Steepest descent algorithm, starting at [-1, 1]^T gives a result of [0.9977, 0.9955]^T in 1698 iterations.\n",
      "For the Steepest descent algorithm, starting at [0, 1]^T gives a result of [0.9965, 0.9931]^T in 1616 iterations.\n",
      "For the Steepest descent algorithm, starting at [2, 1]^T gives a result of [0.9965, 0.9931]^T in 1366 iterations.\n",
      "\n",
      "Newton Method\n",
      "-------------\n",
      "For the Newton algorithm, starting at [-1, 1]^T gives a result of [1.0000, 1.0000]^T in 4 iterations.\n",
      "For the Newton algorithm, starting at [0, 1]^T gives a result of [1.0000, 1.0000]^T in 7 iterations.\n",
      "For the Newton algorithm, starting at [2, 1]^T gives a result of [1.0000, 1.0000]^T in 7 iterations.\n",
      "\n",
      "Damped Newton\n",
      "-------------\n",
      "For the Damped Newton algorithm, starting at [-1, 1]^T gives a result of [1.0000, 1.0000]^T in 10 iterations.\n",
      "For the Damped Newton algorithm, starting at [0, 1]^T gives a result of [1.0000, 1.0000]^T in 8 iterations.\n",
      "For the Damped Newton algorithm, starting at [2, 1]^T gives a result of [1.0000, 1.0000]^T in 7 iterations.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Question 6.9 from the textbook.\n",
    "\n",
    "Write a program to find a minimum of\n",
    "Rosenbrock’s function,\n",
    "f (x 1 , x 2 ) = 100(x 2 − x 21 ) 2 + (1 − x 1 ) 2\n",
    "using each of the following methods:\n",
    "(a) Steepest descent\n",
    "(b) Newton\n",
    "(c) Damped Newton (Newton’s method with\n",
    "a line search)\n",
    "You should try each of the methods from each\n",
    "T\n",
    "of the three starting points x 0 = [ −1 1 ] ,\n",
    "T\n",
    "T\n",
    "[ 0 1 ] , and [ 2 1 ] . For any line searches\n",
    "and linear system solutions required, you may\n",
    "use either library routines or routines of your\n",
    "own design. Plot the path taken in the plane\n",
    "by the approximate solutions for each method\n",
    "from each starting point.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def rosenblock(x):\n",
    "    return 100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0]) ** 2\n",
    "\n",
    "\n",
    "def rosenblock_gradient(x):\n",
    "    return np.array([[400 * (x[0,0] ** 3 - x[0,0] * x[1,0]) + 2 * x[0,0] - 2],\n",
    "                     [200 * (x[1,0] - x[0,0] ** 2)]])\n",
    "\n",
    "def rosenblock_hessian(x):\n",
    "    return np.array([[1200 * x[0, 0] ** 2 - 400 * x[1,0] + 2,\n",
    "                      -400 * x[0,0]],\n",
    "                     [-400 * x[0, 0],\n",
    "                      200]])\n",
    "\n",
    "\n",
    "def alpha_min_steepest_descent(x):\n",
    "    \"\"\"Return a function which can be used for the minimization of alpha.\"\"\"\n",
    "    return lambda alpha: rosenblock(x - alpha * rosenblock_gradient(x))\n",
    "\n",
    "\n",
    "def steepest_descent(x0):\n",
    "    \"\"\"Implementation of steepest descent.\"\"\"    \n",
    "    xk = np.copy(x0)\n",
    "    change = 1\n",
    "    count = 0\n",
    "    while change > 0.0:\n",
    "        alpha = minimize(alpha_min_steepest_descent(xk), np.array([0]))\n",
    "        adjust = alpha.x * rosenblock_gradient(xk)\n",
    "        xk = xk - adjust\n",
    "        change = max(abs(adjust))\n",
    "        count += 1\n",
    "    return xk, count\n",
    "\n",
    "\n",
    "def newton_method(x0):\n",
    "    xk = np.copy(x0)\n",
    "    change = 1\n",
    "    count = 0\n",
    "    while change > 0.0:\n",
    "        hess = rosenblock_hessian(xk)\n",
    "        grad = rosenblock_gradient(xk)\n",
    "        sk = np.linalg.solve(hess, -grad)\n",
    "        change = max(abs(sk))\n",
    "        xk = xk + sk\n",
    "        count += 1\n",
    "    return xk, count\n",
    "\n",
    "\n",
    "def damped_newton(x0):\n",
    "    xk = np.copy(x0)\n",
    "    change = 1000\n",
    "    count = 0\n",
    "    while change > 0.0:\n",
    "        hess = rosenblock_hessian(xk)\n",
    "        grad = rosenblock_gradient(xk)\n",
    "        sk = np.linalg.solve(hess, -grad)\n",
    "        f = lambda alpha: rosenblock(x + alpha * sk)\n",
    "        if change > 10:\n",
    "            alpha = minimize(f, np.array([1])).x\n",
    "        else:\n",
    "            alpha = 1\n",
    "        change = max(abs(alpha * sk))\n",
    "        xk = xk + alpha * sk\n",
    "        count += 1\n",
    "    return xk, count\n",
    "\n",
    "\n",
    "def ez_print(alg_name, input_arr, result):\n",
    "    print(\"For the {0} algorithm, starting at [{1}, {2}]^T gives a result of [{3:.4f}, {4:.4f}]^T in {5} iterations.\".format(\n",
    "        alg_name, input_arr[0,0], input_arr[1,0], result[0][0, 0], result[0][1, 0], result[1]))\n",
    "\n",
    "    \n",
    "# Test cases.\n",
    "test1 = np.array([[-1], [1]])\n",
    "test2 = np.array([[0], [1]])\n",
    "test3 = np.array([[2], [1]])\n",
    "\n",
    "print(\"Steepest Descent\\n----------------\")\n",
    "ez_print(\"Steepest descent\", test1, steepest_descent(test1))\n",
    "ez_print(\"Steepest descent\", test2, steepest_descent(test2))\n",
    "ez_print(\"Steepest descent\", test3, steepest_descent(test3))\n",
    "\n",
    "print(\"\\nNewton Method\\n-------------\")\n",
    "ez_print(\"Newton\", test1, newton_method(test1))\n",
    "ez_print(\"Newton\", test2, newton_method(test2))\n",
    "ez_print(\"Newton\", test3, newton_method(test3))\n",
    "\n",
    "print(\"\\nDamped Newton\\n-------------\")\n",
    "ez_print(\"Damped Newton\", test1, damped_newton(test1))\n",
    "ez_print(\"Damped Newton\", test2, damped_newton(test2))\n",
    "ez_print(\"Damped Newton\", test3, damped_newton(test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6.11\n",
    "\n",
    "This took longer than the newton methods for the rosenblock function, but it was considerably better than steepest descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the bfgs algorithm, starting at [-1, 1]^T gives a result of [1.0000, 1.0000]^T in 126 iterations.\n",
      "For the bfgs algorithm, starting at [0, 1]^T gives a result of [1.0000, 1.0000]^T in 40 iterations.\n",
      "For the bfgs algorithm, starting at [2, 1]^T gives a result of [1.0000, 1.0000]^T in 47 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mw/Documents/NUM_METHODS/lib/python3.5/site-packages/ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "\"\"\"BFGS method for unconstrained minimization.\n",
    "\n",
    "For the purposes of this exercise, you may refactor the resulting\n",
    "matrix B at each iteration.\n",
    "\n",
    "Use an initial value of Bo = I, but you might also wish to include\n",
    "an option to compute a finite difference approximation to the\n",
    "Hessian of the objective function to use as the initial Bo.\n",
    "\n",
    "You may wish to include a line search to enhance the robustness\n",
    "of your routine.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def bfgs(grad_fun, x0):\n",
    "    Bk = np.matrix(np.eye(2))\n",
    "    xk = np.matrix(x0)\n",
    "    change = 1000\n",
    "    count = 0\n",
    "    while change > 0.0:\n",
    "        sk = np.matrix(np.linalg.solve(Bk, -grad_fun(xk)))\n",
    "        change = max(abs(sk))\n",
    "        xk_1 = np.matrix(xk + sk)\n",
    "        yk = np.matrix(grad_fun(xk_1) - grad_fun(xk))\n",
    "        Bk = Bk + (yk * yk.T) / (yk.T * sk) - (Bk * sk * sk.T * Bk) / (sk.T * Bk * sk)\n",
    "        xk = xk_1\n",
    "        count += 1\n",
    "    return xk, count\n",
    "\n",
    "\n",
    "def rosenblock_gradient(x):\n",
    "    return np.array([[400 * (x[0,0] ** 3 - x[0,0] * x[1,0]) + 2 * x[0,0] - 2],\n",
    "                     [200 * (x[1,0] - x[0,0] ** 2)]])\n",
    "\n",
    "\n",
    "def ez_print(alg_name, input_arr, result):\n",
    "    print(\"For the {0} algorithm, starting at [{1}, {2}]^T gives a result of [{3:.4f}, {4:.4f}]^T in {5} iterations.\".format(\n",
    "        alg_name, input_arr[0,0], input_arr[1,0], result[0][0, 0], result[0][1, 0], result[1]))\n",
    "\n",
    "\n",
    "test1 = np.array([[-1], [1]])\n",
    "test2 = np.array([[0], [1]])\n",
    "test3 = np.array([[2], [1]])\n",
    "    \n",
    "ez_print(\"bfgs\", test1, bfgs(rosenblock_gradient, test1))\n",
    "ez_print(\"bfgs\", test2, bfgs(rosenblock_gradient, test2))\n",
    "ez_print(\"bfgs\", test3, bfgs(rosenblock_gradient, test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of optimization of the first system is [0.9999999999999989, 1.9148542173991927]\n",
      "The result of optimization of the first system is [-0.31490834216133096, 0.7592652821738979]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Use library routine to solve overdetermined nonlinear equations.\"\"\"\n",
    "\n",
    "import scipy as sp\n",
    "from math import sin, cos\n",
    "\n",
    "# Part a\n",
    "def system_of_equations_a(x):\n",
    "    return np.array([x[0] ** 2 + x[1] ** 2 - 2,\n",
    "                    (x[0] - 2) ** 2 + x[1] ** 2 - 2,\n",
    "                    (x[0] - 1) ** 2 + x[1] ** 2 - 9])\n",
    "\n",
    "\n",
    "def system_of_equations_b(x):\n",
    "    return np.array([x[0] ** 2 + x[1] ** 2 + x[0] * x[1],\n",
    "                     sin(x[0]) ** 2,\n",
    "                     cos(x[1]) ** 2])\n",
    "\n",
    "\n",
    "print(\"The result of optimization of the first system is [{}, {}]\".format(*sp.optimize.least_squares(system_of_equations_a, np.array([1, 1])).x))\n",
    "print(\"The result of optimization of the first system is [{}, {}]\".format(*sp.optimize.least_squares(system_of_equations_b, np.array([1, 1])).x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incomplete Questions\n",
    "\n",
    "## Chapter 5\n",
    "5.13, 5.17\n",
    "## Chapter 6\n",
    "6.12, 6.19(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
